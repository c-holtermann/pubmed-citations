"""
PubMed Results Analysis and Wiki Table Generation Script

This script performs analysis on PubMed results stored in an XML file (generated by a separate script)
and templates stored in a text file. It cleans up template information and creates a MediaWiki table
displaying the publication details and their reception.

Template data should look like this:

{{Literatur| DOI = 10.1016/j.jclinepi.2017.04.026| ISSN = 1878-5921| Band = 89| Seiten = 218-235| Autor=David S. Riley, Melissa S. Barber, Gunver S. Kienle, Jeffrey K. Aronson, Tido von Schoen-Angerer, Peter Tugwell, Helmut Kiene, Mark Helfand, Douglas G. Altman, Harold Sox, Paul G. Werthmann, David Moher, Richard A. Rison, Larissa Shamseer, Christian A. Koch, Gordon H. Sun, Patrick Hanaway, Nancy L. Sudak, Marietta Kaszkin-Bettag, James E. Carpenter, Joel J. Gagnier| Titel = CARE guidelines for case reports: explanation and elaboration document| Sammelwerk = Journal of Clinical Epidemiology| Sprache = eng| Datum = 2017-09| undefined = 28529185}}

Author: Christoph Holtermann

Date: 2023-12-21

License: GNU GENERAL PUBLIC LICENSE Version 3
"""

# Define input file names
XML_FILE_PATH = "pubmed_results.xml"
TEMPLATE_FILE_PATH = "zotero_export.txt"

# Define output file name
OUTPUT_FILE_PATH = "mediawiki_table_output.txt"

import re
import xml.etree.ElementTree as ET

def parse_templates(file_path):
    """
    Parse template information from a text file.

    Parameters:
    - file_path (str): Path to the text file containing templates.

    Returns:
    - dict: Dictionary mapping keys to template values.
    """
    with open(file_path, 'r', encoding='utf-8') as file:
        content = file.read()

    # The regular expression pattern for the "PMID" key
    pmid_pattern = re.compile(r'\bPMID\s*=\s*(\d+)\b')

    # Find matches in the text
    matches = pmid_pattern.findall(content)

    # Extract the content between {{ and }}
    templates_content = re.findall(r'{{(.*?)}}', content, re.DOTALL)

    # Create a dictionary with the "PMID" key and its corresponding value
    result_dict = {str(matches[i]): "{{"+templates_content[i].strip()+"}}" for i in range(len(matches))}

    return result_dict

def cleanup(results):
    """
    Clean up template results by replacing specific patterns.

    Parameters:
    - results (dict): Dictionary containing template results.
    """
    for key in results.keys():
        result = results[key]

        # Replace "| undefined = ..." with "| PMID = ..."
        result = re.sub(r'\| undefined = ([^|}]*)', r'| PMID = \1', result)

        # Replace "Sprache = ger" with "Sprache = de"
        result = re.sub(r'Sprache = ger', 'Sprache = de', result)

        # Replace "Sprache = eng" with "Sprache = en"
        result = re.sub(r'Sprache = eng', 'Sprache = en', result)

        # Replace "Sprache = nor" with "Sprache = no"
        result = re.sub(r'Sprache = nor', 'Sprache = no', result)

        # Update the value in the dictionary
        results[key] = result

def parse_xml(xml_file):
    """
    Parse XML file containing PubMed results.

    Parameters:
    - xml_file (str): Path to the XML file.

    Returns:
    - dict: Dictionary containing parsed PubMed results.
    """
    tree = ET.parse(xml_file)
    root = tree.getroot()

    results = {}

    for publication in root.iter('Publication'):
        pubmed_id = publication.get('PubMedID')
        citing_papers = [citing.get('PubMedID') for citing in publication.iter('CitingPaper')]
        results[pubmed_id] = {'Citations': len(citing_papers), 'CitingPapers': citing_papers}

    return results

def create_mediawiki_table(output_file, results, templates):
    """
    Create a MediaWiki table and write it to a file.

    Parameters:
    - output_file (str): Path to the output file.
    - results (dict): Dictionary containing PubMed results.
    - templates (dict): Dictionary containing template results.
    """
    with open(output_file, 'w', encoding='utf-8') as file:
        file.write('{| class="wikitable"\n')
        file.write('|+ Publications and Reception H. Kiene\n')
        file.write('|-\n')
        file.write('! Number !! Publication !! Number of citing ' +\
                   'publications !! Citing Publications ' +\
                   '(limited to ten)\n')

        for idx, (pubmed_id, data) in enumerate(results.items(), start=1):
            template = templates.get(pubmed_id, '')
            citations = data['Citations']
            citing_papers_links = []

            baselink = 'https://pubmed.ncbi.nlm.nih.gov/'
            cited_link = \
              f'{baselink}?linkname=pubmed_pubmed_citedin&from_uid={pubmed_id}'

            cropped = len(data['CitingPapers']) > 9

            # Limit to the first ten results
            for idx2, citing_paper in enumerate(
                    data['CitingPapers'][:10], start=1):
                citing_papers_links.append(
                    f'[{baselink}{citing_paper} {idx2}]')

            citing_papers_html = ', '.join(citing_papers_links)

            if cropped:
                citing_papers_html += f", [{cited_link} ...]"

            citations_column = f'[{cited_link} {citations}]'

            file.write(f'|-\n| {idx} || {template} || {citations_column} || {citing_papers_html}\n')

        file.write('|}\n')

if __name__ == "__main__":
    # Analyze results from the XML file
    xml_results = parse_xml(XML_FILE_PATH)

    # Analyze results from the template file
    template_results = parse_templates(TEMPLATE_FILE_PATH)

    # Clean up template results
    cleanup(template_results)

    # Create the MediaWiki table and write to file
    create_mediawiki_table(OUTPUT_FILE_PATH, xml_results, template_results)

